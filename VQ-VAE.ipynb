{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "batch_size =18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_img(img):\n",
    "  # Convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.io.decode_jpeg(img, channels=3)\n",
    "  # Resize the image to the desired size\n",
    "  return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "\n",
    "  # Load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img,\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=1000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files('data_dir''*/*', shuffle=False)\n",
    "val_size = int(tf.data.experimental.cardinality(list_ds).numpy() * 0.3)\n",
    "\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n",
    "\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize. This means give me an embedding codebook with \n",
    "        # num_embedding codes each of embedding_dim dimensions. for instance for the default paramenters \n",
    "        # num_embeddings = 64 and embedding_dims = 16 this will give me a (16,64) matrix 64 vectors of dim 16\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "        \n",
    "        \n",
    "\n",
    "        # Quantization.\n",
    "        \n",
    "        # Get the index of the closest codebook vector for each of the HxW codebook vectors\n",
    "        encoding_indices = self.get_code_indices(flattened)  \n",
    "        \n",
    "        # Encode in a onehot matrix and retrieve the corresponding codebook with a matrix multiplicatino\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)   \n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        # I think this could just be a transpose operation\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        # Similarity is a matrix that compares ALL the encoder codes with all the embedding codes. \n",
    "        # Thus the shape becomes \n",
    "        # (#HxW,EmbeddingDimensionaltiy) x (#EmbeddingDimensionaltiy,#NumCodebookVecs) : (#HxW,NumCodebookVecs)\n",
    "        # (#HxW,16) x (16,64) : (#HxW,64)\n",
    "        \n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        \n",
    "        # Distances outputs  (#HXW,64)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        # For each of the #HXW code this next line selects the 1-out-of-64 minimum distance\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(\n",
    "        encoder_inputs\n",
    "    )\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16             # The number \n",
    "num_embeddings = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the components\n",
    "\n",
    "encoder = get_encoder(num_embeddings)\n",
    "vq_layer = VectorQuantizer(num_embeddings, embedding_dim, name=\"vector_quantizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = keras.Input(shape=(7, 7, embedding_dim))\n",
    "quantized_latents = vq_layer(encoder_outputs)\n",
    "qunatitized = keras.Model(encoder_outputs, quantized_latents, name=\"vq_vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = tf.shape(encoder_outputs)\n",
    "flattened = tf.reshape(encoder_outputs, [-1,embedding_dim])\n",
    "\n",
    "print(flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64)\n"
     ]
    }
   ],
   "source": [
    "w_init = tf.random_uniform_initializer()\n",
    "embeddings = tf.Variable(\n",
    "            initial_value=w_init(shape=(embedding_dim, num_embeddings), dtype=\"float32\"),  \n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'embeddings_vqvae:0' shape=(16, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "similarity = tf.matmul(flattened, embeddings)\n",
    "print(similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "distances = (\n",
    "    tf.reduce_sum(flattened ** 2, axis=1, keepdims=True)\n",
    "    + tf.reduce_sum(embeddings ** 2, axis=0)\n",
    "    - 2 * similarity\n",
    ")\n",
    "\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None,)\n"
     ]
    }
   ],
   "source": [
    "encoding_indices = tf.argmin(distances, axis=1)\n",
    "\n",
    "print(encoding_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened.shape         # In this case a Vector of BHWC of Batchx7x7x16 will be flatten to 49x16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\castr\\Documents\\Generative_AI\\VQ-VAE.ipynb Cell 18\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/castr/Documents/Generative_AI/VQ-VAE.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m quantized_latents \u001b[39m=\u001b[39m vq_layer(encoder_outputs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/castr/Documents/Generative_AI/VQ-VAE.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reconstructions \u001b[39m=\u001b[39m decoder(quantized_latents)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "quantized_latents = vq_layer(encoder_outputs)\n",
    "reconstructions = decoder(quantized_latents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet Enccoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU,Add\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_encoder(input_shape, num_filters, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(num_filters, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # downsample via strided convolutions\n",
    "    filters = [num_filters, num_filters*2, num_filters*4, num_filters*8]\n",
    "    size = len(filters)\n",
    "    for i in range(size):\n",
    "        for j in range(2):\n",
    "            # first block of each layer uses stride 2\n",
    "            strides = 2 if j == 0 else 1\n",
    "            x = resnet_block(x, filters[i], strides=strides)\n",
    "\n",
    "    # final conv layer\n",
    "    x = Conv2D(latent_dim, kernel_size=1, strides=1)(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_block(inputs, filters, strides=1):\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "\n",
    "    shortcut = inputs\n",
    "    if strides != 1 or inputs.shape[3] != filters:\n",
    "        shortcut = Conv2D(filters, kernel_size=1, strides=strides, padding='valid')(inputs)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_351 (Conv2D)            (None, 64, 64, 64)   9472        ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_333 (Batch  (None, 64, 64, 64)  256         ['conv2d_351[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 64, 64, 64)   0           ['batch_normalization_333[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_334 (Batch  (None, 64, 64, 64)  256         ['leaky_re_lu_49[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_50 (LeakyReLU)     (None, 64, 64, 64)   0           ['batch_normalization_334[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_352 (Conv2D)            (None, 32, 32, 64)   36928       ['leaky_re_lu_50[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_335 (Batch  (None, 32, 32, 64)  256         ['conv2d_352[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_51 (LeakyReLU)     (None, 32, 32, 64)   0           ['batch_normalization_335[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_353 (Conv2D)            (None, 32, 32, 64)   36928       ['leaky_re_lu_51[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_354 (Conv2D)            (None, 32, 32, 64)   4160        ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " add_100 (Add)                  (None, 32, 32, 64)   0           ['conv2d_353[0][0]',             \n",
      "                                                                  'conv2d_354[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_336 (Batch  (None, 32, 32, 64)  256         ['add_100[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 32, 32, 64)   0           ['batch_normalization_336[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_355 (Conv2D)            (None, 32, 32, 64)   36928       ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_337 (Batch  (None, 32, 32, 64)  256         ['conv2d_355[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 32, 32, 64)   0           ['batch_normalization_337[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_356 (Conv2D)            (None, 32, 32, 64)   36928       ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " add_101 (Add)                  (None, 32, 32, 64)   0           ['conv2d_356[0][0]',             \n",
      "                                                                  'add_100[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_338 (Batch  (None, 32, 32, 64)  256         ['add_101[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_54 (LeakyReLU)     (None, 32, 32, 64)   0           ['batch_normalization_338[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_357 (Conv2D)            (None, 16, 16, 128)  73856       ['leaky_re_lu_54[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_339 (Batch  (None, 16, 16, 128)  512        ['conv2d_357[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_55 (LeakyReLU)     (None, 16, 16, 128)  0           ['batch_normalization_339[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_358 (Conv2D)            (None, 16, 16, 128)  147584      ['leaky_re_lu_55[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_359 (Conv2D)            (None, 16, 16, 128)  8320        ['add_101[0][0]']                \n",
      "                                                                                                  \n",
      " add_102 (Add)                  (None, 16, 16, 128)  0           ['conv2d_358[0][0]',             \n",
      "                                                                  'conv2d_359[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_340 (Batch  (None, 16, 16, 128)  512        ['add_102[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16, 16, 128)  0           ['batch_normalization_340[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_360 (Conv2D)            (None, 16, 16, 128)  147584      ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_341 (Batch  (None, 16, 16, 128)  512        ['conv2d_360[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 16, 16, 128)  0           ['batch_normalization_341[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_361 (Conv2D)            (None, 16, 16, 128)  147584      ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " add_103 (Add)                  (None, 16, 16, 128)  0           ['conv2d_361[0][0]',             \n",
      "                                                                  'add_102[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_342 (Batch  (None, 16, 16, 128)  512        ['add_103[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_58 (LeakyReLU)     (None, 16, 16, 128)  0           ['batch_normalization_342[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_362 (Conv2D)            (None, 8, 8, 256)    295168      ['leaky_re_lu_58[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_343 (Batch  (None, 8, 8, 256)   1024        ['conv2d_362[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_59 (LeakyReLU)     (None, 8, 8, 256)    0           ['batch_normalization_343[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_363 (Conv2D)            (None, 8, 8, 256)    590080      ['leaky_re_lu_59[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_364 (Conv2D)            (None, 8, 8, 256)    33024       ['add_103[0][0]']                \n",
      "                                                                                                  \n",
      " add_104 (Add)                  (None, 8, 8, 256)    0           ['conv2d_363[0][0]',             \n",
      "                                                                  'conv2d_364[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_344 (Batch  (None, 8, 8, 256)   1024        ['add_104[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 8, 8, 256)    0           ['batch_normalization_344[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_365 (Conv2D)            (None, 8, 8, 256)    590080      ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_345 (Batch  (None, 8, 8, 256)   1024        ['conv2d_365[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 8, 8, 256)    0           ['batch_normalization_345[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_366 (Conv2D)            (None, 8, 8, 256)    590080      ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " add_105 (Add)                  (None, 8, 8, 256)    0           ['conv2d_366[0][0]',             \n",
      "                                                                  'add_104[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_346 (Batch  (None, 8, 8, 256)   1024        ['add_105[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_62 (LeakyReLU)     (None, 8, 8, 256)    0           ['batch_normalization_346[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_367 (Conv2D)            (None, 4, 4, 512)    1180160     ['leaky_re_lu_62[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_347 (Batch  (None, 4, 4, 512)   2048        ['conv2d_367[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_63 (LeakyReLU)     (None, 4, 4, 512)    0           ['batch_normalization_347[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_368 (Conv2D)            (None, 4, 4, 512)    2359808     ['leaky_re_lu_63[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_369 (Conv2D)            (None, 4, 4, 512)    131584      ['add_105[0][0]']                \n",
      "                                                                                                  \n",
      " add_106 (Add)                  (None, 4, 4, 512)    0           ['conv2d_368[0][0]',             \n",
      "                                                                  'conv2d_369[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_348 (Batch  (None, 4, 4, 512)   2048        ['add_106[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 4, 4, 512)    0           ['batch_normalization_348[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_370 (Conv2D)            (None, 4, 4, 512)    2359808     ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_349 (Batch  (None, 4, 4, 512)   2048        ['conv2d_370[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 4, 4, 512)    0           ['batch_normalization_349[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_371 (Conv2D)            (None, 4, 4, 512)    2359808     ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " add_107 (Add)                  (None, 4, 4, 512)    0           ['conv2d_371[0][0]',             \n",
      "                                                                  'add_106[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_372 (Conv2D)            (None, 4, 4, 128)    65664       ['add_107[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,255,360\n",
      "Trainable params: 11,248,448\n",
      "Non-trainable params: 6,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = resnet_encoder(input_shape=(128, 128, 3), num_filters=64, latent_dim=128)\n",
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\castr\\.conda\\envs\\work-portable\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\castr\\.conda\\envs\\work-portable\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: graphviz in c:\\users\\castr\\.conda\\envs\\work-portable\\lib\\site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_model() got an unexpected keyword argument 'fontsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\castr\\Documents\\Generative_AI\\VQ-VAE.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/castr/Documents/Generative_AI/VQ-VAE.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mplot_model(encoder_model, to_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mencoder.png\u001b[39;49m\u001b[39m'\u001b[39;49m, show_shapes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,show_layer_names\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,dpi\u001b[39m=\u001b[39;49m\u001b[39m180\u001b[39;49m,fontsize\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_model() got an unexpected keyword argument 'fontsize'"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(encoder_model, to_file='encoder.png', show_shapes=True,show_layer_names=False,dpi=180,fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work-portable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
